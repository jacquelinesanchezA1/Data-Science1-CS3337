{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac027c74",
   "metadata": {},
   "source": [
    "## Lab4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce8dd15",
   "metadata": {},
   "source": [
    "### 1. State the difference and similarity of GD and SGD, give each two situitions, one should use GD, the other should use SGD. Explain why. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3c0925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0314c55e",
   "metadata": {},
   "source": [
    "Write your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d63c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebbc6ce7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m STAB \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmart_grid_stability_augmented.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m STAB \u001b[38;5;241m=\u001b[39m STAB\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstab\u001b[39m\u001b[38;5;124m'\u001b[39m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m STAB\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "STAB = pd.read_csv(\"smart_grid_stability_augmented.csv\")\n",
    "STAB = STAB.drop('stab', axis = 1)\n",
    "STAB.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914bb207",
   "metadata": {},
   "source": [
    "### 2. Seperate the STAB dataset to train and test, stabf is y, the others are x, make test dataset 30% of the total dataset (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b7ed17b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STAB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Separate features (X) and target (y)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m STAB\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstabf\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m STAB[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstabf\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Split the original dataset into training and testing sets (30% for testing)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'STAB' is not defined"
     ]
    }
   ],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = STAB.drop('stabf', axis=1)\n",
    "y = STAB['stabf']\n",
    "\n",
    "# Split the original dataset into training and testing sets (30% for testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5497f7ac",
   "metadata": {},
   "source": [
    "### 3. Apply Random Forest to this dataset, predict stabf, show the accuracy. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f77eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Train the Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# 2. make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# 3. evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f'Random Forest Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc85a2d",
   "metadata": {},
   "source": [
    "### 4. Apply SVM to this dataset, predict stabf, show the accuracy. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc8d2aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. train the SVM model\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# 2. make predictions\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "# 3. evaluate the model\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "\n",
    "print(f'SVM Accuracy: {accuracy_svm:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b2f63d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic_Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Ash_Alcanity</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total_Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid_Phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color_Intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Customer_Segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol  Malic_Acid   Ash  Ash_Alcanity  Magnesium  Total_Phenols  \\\n",
       "0    14.23        1.71  2.43          15.6        127           2.80   \n",
       "1    13.20        1.78  2.14          11.2        100           2.65   \n",
       "2    13.16        2.36  2.67          18.6        101           2.80   \n",
       "3    14.37        1.95  2.50          16.8        113           3.85   \n",
       "4    13.24        2.59  2.87          21.0        118           2.80   \n",
       "\n",
       "   Flavanoids  Nonflavanoid_Phenols  Proanthocyanins  Color_Intensity   Hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   OD280  Proline  Customer_Segment  \n",
       "0   3.92     1065                 1  \n",
       "1   3.40     1050                 1  \n",
       "2   3.17     1185                 1  \n",
       "3   3.45     1480                 1  \n",
       "4   2.93      735                 1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = pd.read_csv(\"Wine.csv\")\n",
    "wine.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61396e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=wine.drop(\"Customer_Segment\",axis=1).values\n",
    "y=wine[\"Customer_Segment\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d2a615",
   "metadata": {},
   "source": [
    "### 5. Standartize the variables X (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17320f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of standardized training data:\n",
      "       tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
      "0  0.944087  1.322404  0.212504  0.236362  0.308582 -0.848871  1.201085   \n",
      "1 -0.819194  1.252164  0.296433 -1.599900  0.635702  1.104413 -1.583831   \n",
      "2  0.997219 -0.107312  0.970398  0.921308 -0.245331  1.314141 -0.051993   \n",
      "3 -1.097213  0.708523  1.105498  0.934111 -1.754412  1.697428  1.352325   \n",
      "4 -0.285885  0.964390  0.784895 -1.501700 -0.703486 -1.616879  1.500220   \n",
      "\n",
      "         p4        g1        g2        g3        g4  \n",
      "0 -0.892750  1.485538  0.208525  0.421753 -0.798097  \n",
      "1 -0.621148  1.454570  1.522550  0.979838 -1.426316  \n",
      "2 -0.833814  1.519562  1.073471  1.634525 -0.450236  \n",
      "3  0.000293 -0.590446 -1.416779 -1.615071  0.908748  \n",
      "4  1.334720 -1.394408  1.080738  0.022626  1.046577  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. standardize the data (fit on training data, transform on both training and testing data)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Display the first 5 rows of the standardized training data\n",
    "print(\"First 5 rows of standardized training data:\")\n",
    "print(pd.DataFrame(X_train_scaled, columns=X_train.columns).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b08d8d",
   "metadata": {},
   "source": [
    "### 6. Use PCA to reduce standartized X to 2 dimensions (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffc24a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of PCA-transformed training data:\n",
      "        PC1       PC2\n",
      "0 -0.472485 -1.522133\n",
      "1 -0.870298 -1.093831\n",
      "2  0.316512 -1.880438\n",
      "3  2.509509  1.677581\n",
      "4  1.014759  0.467995\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# applying PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "# storing them so that i can refer back to them!\n",
    "df_X_train_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2'])\n",
    "df_X_test_pca = pd.DataFrame(X_test_pca, columns=['PC1', 'PC2'])\n",
    "\n",
    "print(\"First 5 rows of PCA-transformed training data:\")\n",
    "print(df_X_train_pca.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d388ca7",
   "metadata": {},
   "source": [
    "### 7. Use LDA to reduce standartized X to 2 dimensions (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ca39d85",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [42000, 142]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m n_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(X_train_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], n_classes \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m lda \u001b[38;5;241m=\u001b[39m LDA(n_components\u001b[38;5;241m=\u001b[39mn_components)\n\u001b[0;32m----> 6\u001b[0m X_train_lda \u001b[38;5;241m=\u001b[39m lda\u001b[38;5;241m.\u001b[39mfit_transform(X_train_scaled, y_train)\n\u001b[1;32m      7\u001b[0m X_test_lda \u001b[38;5;241m=\u001b[39m lda\u001b[38;5;241m.\u001b[39mtransform(X_test_scaled)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Store the LDA-transformed data in DataFrames\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:881\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m--> 881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:575\u001b[0m, in \u001b[0;36mLinearDiscriminantAnalysis.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m    573\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 575\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    576\u001b[0m     X, y, ensure_min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m[xp\u001b[38;5;241m.\u001b[39mfloat64, xp\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m    577\u001b[0m )\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m unique_labels(y)\n\u001b[1;32m    579\u001b[0m n_samples, _ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1124\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[1;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m-> 1124\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [42000, 142]"
     ]
    }
   ],
   "source": [
    "# Apply LDA\n",
    "y_train_series = pd.Series(y_train)  # Convert y_train to a pandas Series\n",
    "n_classes = len(y_train_series.unique())\n",
    "n_components = min(X_train_scaled.shape[1], n_classes - 1)\n",
    "lda = LDA(n_components=n_components)\n",
    "X_train_lda = lda.fit_transform(X_train_scaled, y_train)\n",
    "X_test_lda = lda.transform(X_test_scaled)\n",
    "\n",
    "# Store the LDA-transformed data in DataFrames\n",
    "columns_lda = [f'LD{i+1}' for i in range(n_components)]\n",
    "df_X_train_lda = pd.DataFrame(X_train_lda, columns=columns_lda)\n",
    "df_X_test_lda = pd.DataFrame(X_test_lda, columns=columns_lda)\n",
    "\n",
    "# Save the LDA-transformed DataFrames to CSV files\n",
    "df_X_train_lda.to_csv('X_train_lda.csv', index=False)\n",
    "df_X_test_lda.to_csv('X_test_lda.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553286b",
   "metadata": {},
   "source": [
    "### 8. Apply logistic regression to standartized X, PCA X and LDA X to predict Y, compare the result. Remember to split train and test sets. Set test sets to 20% of whole data. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47fb3020",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Step 5: Split the datasets into training and testing sets (20% for testing)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m X_train_pca, X_test_pca, y_train_pca, y_test_pca \u001b[38;5;241m=\u001b[39m train_test_split(X_pca, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      7\u001b[0m X_train_lda, X_test_lda, y_train_lda, y_test_lda \u001b[38;5;241m=\u001b[39m train_test_split(X_lda, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Step 7: Train logistic regression models on standardized, PCA, and LDA data\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_pca' is not defined"
     ]
    }
   ],
   "source": [
    "#Train logistic regression models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Step 5: Split the datasets into training and testing sets (20% for testing)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "X_train_lda, X_test_lda, y_train_lda, y_test_lda = train_test_split(X_lda, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Train logistic regression models on standardized, PCA, and LDA data\n",
    "log_reg_standard = LogisticRegression(random_state=42)\n",
    "log_reg_pca = LogisticRegression(random_state=42)\n",
    "log_reg_lda = LogisticRegression(random_state=42)\n",
    "\n",
    "log_reg_standard.fit(X_train_scaled, y_train)\n",
    "log_reg_pca.fit(X_train_pca, y_train_pca)\n",
    "log_reg_lda.fit(X_train_lda, y_train_lda)\n",
    "\n",
    "# Step 8: Make predictions and evaluate the models\n",
    "y_pred_standard = log_reg_standard.predict(X_test_scaled)\n",
    "y_pred_pca = log_reg_pca.predict(X_test_pca)\n",
    "y_pred_lda = log_reg_lda.predict(X_test_lda)\n",
    "\n",
    "accuracy_standard = accuracy_score(y_test, y_pred_standard)\n",
    "accuracy_pca = accuracy_score(y_test_pca, y_pred_pca)\n",
    "accuracy_lda = accuracy_score(y_test_lda, y_pred_lda)\n",
    "\n",
    "# Step 9: Compare the results\n",
    "print(f'Logistic Regression Accuracy on Standardized Data: {accuracy_standard:.2f}')\n",
    "print(f'Logistic Regression Accuracy on PCA Data: {accuracy_pca:.2f}')\n",
    "print(f'Logistic Regression Accuracy on LDA Data: {accuracy_lda:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ebce77",
   "metadata": {},
   "source": [
    "### 9. Apply feature select to the STAB data.(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b6a608c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: Index(['tau1', 'tau2', 'tau3', 'tau4', 'g2'], dtype='object')\n",
      "Feature Ranking: [1 1 1 1 8 6 7 5 4 1 2 3]\n",
      "Logistic Regression Accuracy with Selected Features: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "# Step 2: Apply feature selection using RFE\n",
    "rfe = RFE(estimator=log_reg, n_features_to_select=5)  # Select 5 features for illustration\n",
    "X_train_rfe = rfe.fit_transform(X_train_scaled, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_scaled)\n",
    "\n",
    "# Get the selected feature indices and their rankings\n",
    "selected_features = X.columns[rfe.support_]\n",
    "ranking = rfe.ranking_\n",
    "\n",
    "# Step 3: Train the logistic regression model\n",
    "log_reg_rfe = LogisticRegression(random_state=42)\n",
    "log_reg_rfe.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Step 4: Make predictions and evaluate the model\n",
    "y_pred_rfe = log_reg_rfe.predict(X_test_rfe)\n",
    "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
    "\n",
    "# Step 5: Display the results\n",
    "print(f'Selected Features: {selected_features}')\n",
    "print(f'Feature Ranking: {ranking}')\n",
    "print(f'Logistic Regression Accuracy with Selected Features: {accuracy_rfe:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9498fe",
   "metadata": {},
   "source": [
    "### 10. Apply SVM to the edited STAB dataset, compare the result with the previous one.(Remember to split the dataset)(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "923364e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy with Selected Features: 0.75\n",
      "SVM Accuracy with Selected Features: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 2: Initialize and train the SVM model with selected features\n",
    "svm_rfe = SVC(kernel='linear', random_state=42)\n",
    "svm_rfe.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Step 3: Make predictions\n",
    "y_pred_svm_rfe = svm_rfe.predict(X_test_rfe)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "accuracy_svm_rfe = accuracy_score(y_test, y_pred_svm_rfe)\n",
    "\n",
    "# Step 5: Compare the results with the logistic regression model\n",
    "print(f'Logistic Regression Accuracy with Selected Features: {accuracy_rfe:.2f}')\n",
    "print(f'SVM Accuracy with Selected Features: {accuracy_svm_rfe:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
