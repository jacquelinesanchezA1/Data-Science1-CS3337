{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac027c74",
   "metadata": {},
   "source": [
    "## Lab4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce8dd15",
   "metadata": {},
   "source": [
    "### 1. State the difference and similarity of GD and SGD, give each two situitions, one should use GD, the other should use SGD. Explain why. (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0314c55e",
   "metadata": {},
   "source": [
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ae1bb",
   "metadata": {},
   "source": [
    "<span style = 'color:green'>\n",
    "Gradient Descent processes the entire dataset for each update, leading to stable but slower convergence, making it suitable for small datasets and convex loss functions. Stochastic Gradient Descent, on the other hand, updates parameters more frequently using individual data points which speeds up convergence but introduces more noise. Both methods aim to minimize loss functions by adjusting model parameters based on gradient calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d63c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c41edd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAB = pd.read_csv(\"smart_grid_stability_augmented.csv\") # just to observe the data before pre-processing it\n",
    "# STAB.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebbc6ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tau1</th>\n",
       "      <th>tau2</th>\n",
       "      <th>tau3</th>\n",
       "      <th>tau4</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>g1</th>\n",
       "      <th>g2</th>\n",
       "      <th>g3</th>\n",
       "      <th>g4</th>\n",
       "      <th>stabf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.959060</td>\n",
       "      <td>3.079885</td>\n",
       "      <td>8.381025</td>\n",
       "      <td>9.780754</td>\n",
       "      <td>3.763085</td>\n",
       "      <td>-0.782604</td>\n",
       "      <td>-1.257395</td>\n",
       "      <td>-1.723086</td>\n",
       "      <td>0.650456</td>\n",
       "      <td>0.859578</td>\n",
       "      <td>0.887445</td>\n",
       "      <td>0.958034</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.304097</td>\n",
       "      <td>4.902524</td>\n",
       "      <td>3.047541</td>\n",
       "      <td>1.369357</td>\n",
       "      <td>5.067812</td>\n",
       "      <td>-1.940058</td>\n",
       "      <td>-1.872742</td>\n",
       "      <td>-1.255012</td>\n",
       "      <td>0.413441</td>\n",
       "      <td>0.862414</td>\n",
       "      <td>0.562139</td>\n",
       "      <td>0.781760</td>\n",
       "      <td>stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.971707</td>\n",
       "      <td>8.848428</td>\n",
       "      <td>3.046479</td>\n",
       "      <td>1.214518</td>\n",
       "      <td>3.405158</td>\n",
       "      <td>-1.207456</td>\n",
       "      <td>-1.277210</td>\n",
       "      <td>-0.920492</td>\n",
       "      <td>0.163041</td>\n",
       "      <td>0.766689</td>\n",
       "      <td>0.839444</td>\n",
       "      <td>0.109853</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.716415</td>\n",
       "      <td>7.669600</td>\n",
       "      <td>4.486641</td>\n",
       "      <td>2.340563</td>\n",
       "      <td>3.963791</td>\n",
       "      <td>-1.027473</td>\n",
       "      <td>-1.938944</td>\n",
       "      <td>-0.997374</td>\n",
       "      <td>0.446209</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.929381</td>\n",
       "      <td>0.362718</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.134112</td>\n",
       "      <td>7.608772</td>\n",
       "      <td>4.943759</td>\n",
       "      <td>9.857573</td>\n",
       "      <td>3.525811</td>\n",
       "      <td>-1.125531</td>\n",
       "      <td>-1.845975</td>\n",
       "      <td>-0.554305</td>\n",
       "      <td>0.797110</td>\n",
       "      <td>0.455450</td>\n",
       "      <td>0.656947</td>\n",
       "      <td>0.820923</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
       "0  2.959060  3.079885  8.381025  9.780754  3.763085 -0.782604 -1.257395   \n",
       "1  9.304097  4.902524  3.047541  1.369357  5.067812 -1.940058 -1.872742   \n",
       "2  8.971707  8.848428  3.046479  1.214518  3.405158 -1.207456 -1.277210   \n",
       "3  0.716415  7.669600  4.486641  2.340563  3.963791 -1.027473 -1.938944   \n",
       "4  3.134112  7.608772  4.943759  9.857573  3.525811 -1.125531 -1.845975   \n",
       "\n",
       "         p4        g1        g2        g3        g4     stabf  \n",
       "0 -1.723086  0.650456  0.859578  0.887445  0.958034  unstable  \n",
       "1 -1.255012  0.413441  0.862414  0.562139  0.781760    stable  \n",
       "2 -0.920492  0.163041  0.766689  0.839444  0.109853  unstable  \n",
       "3 -0.997374  0.446209  0.976744  0.929381  0.362718  unstable  \n",
       "4 -0.554305  0.797110  0.455450  0.656947  0.820923  unstable  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STAB = pd.read_csv(\"smart_grid_stability_augmented.csv\")\n",
    "STAB = STAB.drop('stab', axis = 1)\n",
    "STAB.head(5) # seems like the response variable is 'stabf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19347789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 13\n"
     ]
    }
   ],
   "source": [
    "num_rows, num_cols = STAB.shape\n",
    "print(num_rows, num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914bb207",
   "metadata": {},
   "source": [
    "### 2. Seperate the STAB dataset to train and test, stabf is y, the others are x, make test dataset 30% of the total dataset (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b7ed17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here. \n",
    "\n",
    "X = STAB.drop('stabf', axis = 1)\n",
    "y = STAB['stabf']\n",
    "\n",
    "X_train, X_test,y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# print(X_train.head())\n",
    "# print(y_train.head())                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a0e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5497f7ac",
   "metadata": {},
   "source": [
    "### 3. Apply Random Forest to this dataset, predict stabf, show the accuracy. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70f77eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# step 1: Import the necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# step 2: Train the Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# step 3: Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# step 4: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# display the accuracy\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc85a2d",
   "metadata": {},
   "source": [
    "### 4. Apply SVM to this dataset, predict stabf, show the accuracy. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc8d2aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Step 1: import libraries\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Step 2: train the svm model \n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: make predictions\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "# Step 4: evaluate the model\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "\n",
    "print(f'SVM Accuracy: {accuracy_svm:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b2f63d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic_Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Ash_Alcanity</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total_Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid_Phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color_Intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Customer_Segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol  Malic_Acid   Ash  Ash_Alcanity  Magnesium  Total_Phenols  \\\n",
       "0    14.23        1.71  2.43          15.6        127           2.80   \n",
       "1    13.20        1.78  2.14          11.2        100           2.65   \n",
       "2    13.16        2.36  2.67          18.6        101           2.80   \n",
       "3    14.37        1.95  2.50          16.8        113           3.85   \n",
       "4    13.24        2.59  2.87          21.0        118           2.80   \n",
       "\n",
       "   Flavanoids  Nonflavanoid_Phenols  Proanthocyanins  Color_Intensity   Hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   OD280  Proline  Customer_Segment  \n",
       "0   3.92     1065                 1  \n",
       "1   3.40     1050                 1  \n",
       "2   3.17     1185                 1  \n",
       "3   3.45     1480                 1  \n",
       "4   2.93      735                 1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = pd.read_csv(\"Wine.csv\")\n",
    "wine.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61396e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=wine.drop(\"Customer_Segment\",axis=1).values\n",
    "y=wine[\"Customer_Segment\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d2a615",
   "metadata": {},
   "source": [
    "### 5. Standartize the variables X (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17320f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
      "0  0.944087  1.322404  0.212504  0.236362  0.308582 -0.848871  1.201085   \n",
      "1 -0.819194  1.252164  0.296433 -1.599900  0.635702  1.104413 -1.583831   \n",
      "2  0.997219 -0.107312  0.970398  0.921308 -0.245331  1.314141 -0.051993   \n",
      "3 -1.097213  0.708523  1.105498  0.934111 -1.754412  1.697428  1.352325   \n",
      "4 -0.285885  0.964390  0.784895 -1.501700 -0.703486 -1.616879  1.500220   \n",
      "\n",
      "         p4        g1        g2        g3        g4  \n",
      "0 -0.892750  1.485538  0.208525  0.421753 -0.798097  \n",
      "1 -0.621148  1.454570  1.522550  0.979838 -1.426316  \n",
      "2 -0.833814  1.519562  1.073471  1.634525 -0.450236  \n",
      "3  0.000293 -0.590446 -1.416779 -1.615071  0.908748  \n",
      "4  1.334720 -1.394408  1.080738  0.022626  1.046577  \n"
     ]
    }
   ],
   "source": [
    "#Write your code here.\n",
    "# step 1: import necessary libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# step 2: invoke scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# step 3: fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# step 4: transform the test data \n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# viewing the standardized data\n",
    "print(pd.DataFrame(X_train_scaled, columns = X_train.columns).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b08d8d",
   "metadata": {},
   "source": [
    "### 6. Use PCA to reduce standartized X to 2 dimensions (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffc24a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        PC1       PC2\n",
      "0 -0.472485 -1.522133\n",
      "1 -0.870298 -1.093831\n",
      "2  0.316512 -1.880438\n",
      "3  2.509509  1.677581\n",
      "4  1.014759  0.467995\n"
     ]
    }
   ],
   "source": [
    "#Write your code here.\n",
    "# 1: import the necessary libraries\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 2: initialize PCA with the number of components (in this case 2 dim)\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "# 3: fit and transform the training data\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# 4: transform the test data\n",
    "x_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(pd.DataFrame(X_train_pca, columns = ['PC1', 'PC2']).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d388ca7",
   "metadata": {},
   "source": [
    "### 7. Use LDA to reduce standartized X to 2 dimensions (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ca39d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        LD1\n",
      "0  1.825980\n",
      "1  0.652440\n",
      "2  2.911761\n",
      "3 -0.364357\n",
      "4  0.401231\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import the necessary library\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# determine the total unique features/columns \n",
    "n_features = len(y_train.unique())\n",
    "# print(n_features)\n",
    "\n",
    "\n",
    "# Determine the number of components for LDA\n",
    "# n_features is the number of features in X_train\n",
    "n_classes = len(y_train.unique())  # Number of unique classes in the target variable\n",
    "n_features = X_train_scaled.shape[1]  # Number of features in the dataset\n",
    "\n",
    "# The number of components for LDA should be the minimum of (number of features - 1) and (number of classes - 1)\n",
    "n_dimension = min(n_features - 1, n_classes - 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# step 2: Initialize LDA with the number of components\n",
    "lda = LDA(n_components=n_dimension)\n",
    "# lda = LDA(n_components= 2)\n",
    "\n",
    "\n",
    "# step 3: fit and transform the training data\n",
    "X_train_lda = lda.fit_transform(X_train_scaled, y_train)\n",
    "\n",
    "# step 4: transform the test datahttp://localhost:8888/notebooks/Labs/Lab4/Lab4.ipynb#\n",
    "X_test_lda = lda.transform(X_test_scaled)\n",
    "\n",
    "# create appropriate column names for the DataFrame\n",
    "columns = [f'LD{i+1}' for i in range(n_dimension)]\n",
    "\n",
    "# 5 rows of the LDA-transformed training data\n",
    "print(pd.DataFrame(X_train_lda, columns=columns).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553286b",
   "metadata": {},
   "source": [
    "### 8. Apply logistic regression to standartized X, PCA X and LDA X to predict Y, compare the result. Remember to split train and test sets. Set test sets to 20% of whole data. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47fb3020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy on Standardized Data: 0.8130952380952381\n",
      "Logistic Regression Accuracy on PCA Data: 0.6617857142857143\n",
      "Logistic Regression Accuracy on LDA Data: 0.8121428571428572\n"
     ]
    }
   ],
   "source": [
    "#write your code here\n",
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets (20% for testing)\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "X_train_pca_lr, X_test_pca_lr, y_train_pca_lr, y_test_pca_lr = train_test_split(X_train_pca, y_train, test_size=0.2, random_state=42)\n",
    "X_train_lda_lr, X_test_lda_lr, y_train_lda_lr, y_test_lda_lr = train_test_split(X_train_lda, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit and predict on standardized data\n",
    "log_reg.fit(X_train_lr, y_train_lr)\n",
    "y_pred_lr = log_reg.predict(X_test_lr)\n",
    "accuracy_lr = accuracy_score(y_test_lr, y_pred_lr)\n",
    "\n",
    "# Fit and predict on PCA reduced data\n",
    "log_reg.fit(X_train_pca_lr, y_train_pca_lr)\n",
    "y_pred_pca_lr = log_reg.predict(X_test_pca_lr)\n",
    "accuracy_pca_lr = accuracy_score(y_test_pca_lr, y_pred_pca_lr)\n",
    "\n",
    "# Fit and predict on LDA reduced data\n",
    "log_reg.fit(X_train_lda_lr, y_train_lda_lr)\n",
    "y_pred_lda_lr = log_reg.predict(X_test_lda_lr)\n",
    "accuracy_lda_lr = accuracy_score(y_test_lda_lr, y_pred_lda_lr)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy on Standardized Data: {accuracy_lr}\")\n",
    "print(f\"Logistic Regression Accuracy on PCA Data: {accuracy_pca_lr}\")\n",
    "print(f\"Logistic Regression Accuracy on LDA Data: {accuracy_lda_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ebce77",
   "metadata": {},
   "source": [
    "### 9. Apply feature select to the STAB data.(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b6a608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "STAB = pd.read_csv(\"smart_grid_stability_augmented.csv\")\n",
    "\n",
    "# Separate features and target\n",
    "X = STAB.drop(['stabf', 'stab'], axis=1)  # Features (excluding 'stabf' and 'stab')\n",
    "y = STAB['stabf']                        # Target variable 'stabf'\n",
    "\n",
    "# Train a Decision Tree Classifier to get feature importances\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select top N features (e.g., top 5)\n",
    "top_n = 5\n",
    "top_features = feature_importances.head(top_n)['Feature'].tolist()\n",
    "# Transform the dataset to keep only the top N features\n",
    "X_selected = X[top_features]\n",
    "print(X_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9498fe",
   "metadata": {},
   "source": [
    "### 10. Apply SVM to the edited STAB dataset, compare the result with the previous one.(Remember to split the dataset)(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "923364e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM on Original Dataset:\n",
      "Accuracy: 0.9288333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      stable       0.92      0.88      0.90      6522\n",
      "    unstable       0.93      0.96      0.94     11478\n",
      "\n",
      "    accuracy                           0.93     18000\n",
      "   macro avg       0.93      0.92      0.92     18000\n",
      "weighted avg       0.93      0.93      0.93     18000\n",
      "\n",
      "\n",
      "SVM on Edited Dataset (Selected Features):\n",
      "Accuracy: 0.8122222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      stable       0.79      0.66      0.72      6522\n",
      "    unstable       0.82      0.90      0.86     11478\n",
      "\n",
      "    accuracy                           0.81     18000\n",
      "   macro avg       0.81      0.78      0.79     18000\n",
      "weighted avg       0.81      0.81      0.81     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "##################### (A) preparing data for training #######################\n",
    "\n",
    "# (1) Separate features and target\n",
    "X = STAB.drop('stabf', axis=1)  # all other features other than 'stabf'\n",
    "y = STAB['stabf'] # target/response var, 'stabf'\n",
    "\n",
    "\n",
    "# (2) split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train_selected, X_test_selected, y_train_selected, y_test_selected = train_test_split(X_selected,# (* X_selected was aquired from the feature selection made in the cell above!)\n",
    "                                                                                        y, \n",
    "                                                                                        test_size=0.3, \n",
    "                                                                                        random_state=42)\n",
    "\n",
    "###################### (B) Train SVM Models ######################\n",
    "\n",
    "# train SVM on the original dataset\n",
    "svm_original = SVC()\n",
    "svm_original.fit(X_train, y_train)\n",
    "y_pred_original = svm_original.predict(X_test)\n",
    "\n",
    "# train SVM on the edited dataset (selected features)\n",
    "svm_selected = SVC()\n",
    "svm_selected.fit(X_train_selected, y_train_selected)\n",
    "y_pred_selected = svm_selected.predict(X_test_selected)\n",
    "\n",
    "\n",
    "\n",
    "####################     (C) Evaluate Models   ####################\n",
    "\n",
    "# Evaluate the SVM model on the original dataset\n",
    "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
    "report_original = classification_report(y_test, y_pred_original)\n",
    "\n",
    "# Evaluate the SVM model on the edited dataset (selected features)\n",
    "accuracy_selected = accuracy_score(y_test_selected, y_pred_selected)\n",
    "report_selected = classification_report(y_test_selected, y_pred_selected)\n",
    "\n",
    "\n",
    "\n",
    "### Step 4: Compare Results\n",
    "\n",
    "print(\"SVM on Original Dataset:\")\n",
    "print(f\"Accuracy: {accuracy_original}\")\n",
    "print(report_original)\n",
    "\n",
    "print(\"\\nSVM on Edited Dataset (Selected Features):\")\n",
    "print(f\"Accuracy: {accuracy_selected}\")\n",
    "print(report_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b747f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3badabe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1200dd1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b065452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4cad55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f270c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
