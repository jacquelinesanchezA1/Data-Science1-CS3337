{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac027c74",
   "metadata": {},
   "source": [
    "## Lab4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce8dd15",
   "metadata": {},
   "source": [
    "### 1. State the difference and similarity of GD and SGD, give each two situitions, one should use GD, the other should use SGD. Explain why. (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0314c55e",
   "metadata": {},
   "source": [
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ae1bb",
   "metadata": {},
   "source": [
    "<span style = 'color:green'>\n",
    "Gradient Descent processes the entire dataset for each update, leading to stable but slower convergence, making it suitable for small datasets and convex loss functions. Stochastic Gradient Descent, on the other hand, updates parameters more frequently using individual data points which speeds up convergence but introduces more noise. Both methods aim to minimize loss functions by adjusting model parameters based on gradient calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d63c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebbc6ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tau1</th>\n",
       "      <th>tau2</th>\n",
       "      <th>tau3</th>\n",
       "      <th>tau4</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>g1</th>\n",
       "      <th>g2</th>\n",
       "      <th>g3</th>\n",
       "      <th>g4</th>\n",
       "      <th>stabf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.959060</td>\n",
       "      <td>3.079885</td>\n",
       "      <td>8.381025</td>\n",
       "      <td>9.780754</td>\n",
       "      <td>3.763085</td>\n",
       "      <td>-0.782604</td>\n",
       "      <td>-1.257395</td>\n",
       "      <td>-1.723086</td>\n",
       "      <td>0.650456</td>\n",
       "      <td>0.859578</td>\n",
       "      <td>0.887445</td>\n",
       "      <td>0.958034</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.304097</td>\n",
       "      <td>4.902524</td>\n",
       "      <td>3.047541</td>\n",
       "      <td>1.369357</td>\n",
       "      <td>5.067812</td>\n",
       "      <td>-1.940058</td>\n",
       "      <td>-1.872742</td>\n",
       "      <td>-1.255012</td>\n",
       "      <td>0.413441</td>\n",
       "      <td>0.862414</td>\n",
       "      <td>0.562139</td>\n",
       "      <td>0.781760</td>\n",
       "      <td>stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.971707</td>\n",
       "      <td>8.848428</td>\n",
       "      <td>3.046479</td>\n",
       "      <td>1.214518</td>\n",
       "      <td>3.405158</td>\n",
       "      <td>-1.207456</td>\n",
       "      <td>-1.277210</td>\n",
       "      <td>-0.920492</td>\n",
       "      <td>0.163041</td>\n",
       "      <td>0.766689</td>\n",
       "      <td>0.839444</td>\n",
       "      <td>0.109853</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.716415</td>\n",
       "      <td>7.669600</td>\n",
       "      <td>4.486641</td>\n",
       "      <td>2.340563</td>\n",
       "      <td>3.963791</td>\n",
       "      <td>-1.027473</td>\n",
       "      <td>-1.938944</td>\n",
       "      <td>-0.997374</td>\n",
       "      <td>0.446209</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.929381</td>\n",
       "      <td>0.362718</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.134112</td>\n",
       "      <td>7.608772</td>\n",
       "      <td>4.943759</td>\n",
       "      <td>9.857573</td>\n",
       "      <td>3.525811</td>\n",
       "      <td>-1.125531</td>\n",
       "      <td>-1.845975</td>\n",
       "      <td>-0.554305</td>\n",
       "      <td>0.797110</td>\n",
       "      <td>0.455450</td>\n",
       "      <td>0.656947</td>\n",
       "      <td>0.820923</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
       "0  2.959060  3.079885  8.381025  9.780754  3.763085 -0.782604 -1.257395   \n",
       "1  9.304097  4.902524  3.047541  1.369357  5.067812 -1.940058 -1.872742   \n",
       "2  8.971707  8.848428  3.046479  1.214518  3.405158 -1.207456 -1.277210   \n",
       "3  0.716415  7.669600  4.486641  2.340563  3.963791 -1.027473 -1.938944   \n",
       "4  3.134112  7.608772  4.943759  9.857573  3.525811 -1.125531 -1.845975   \n",
       "\n",
       "         p4        g1        g2        g3        g4     stabf  \n",
       "0 -1.723086  0.650456  0.859578  0.887445  0.958034  unstable  \n",
       "1 -1.255012  0.413441  0.862414  0.562139  0.781760    stable  \n",
       "2 -0.920492  0.163041  0.766689  0.839444  0.109853  unstable  \n",
       "3 -0.997374  0.446209  0.976744  0.929381  0.362718  unstable  \n",
       "4 -0.554305  0.797110  0.455450  0.656947  0.820923  unstable  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STAB = pd.read_csv(\"smart_grid_stability_augmented.csv\")\n",
    "STAB = STAB.drop('stab', axis = 1)\n",
    "STAB.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11481402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 13\n"
     ]
    }
   ],
   "source": [
    "num_rows, num_cols = STAB.shape\n",
    "print(num_rows, num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914bb207",
   "metadata": {},
   "source": [
    "### 2. Seperate the STAB dataset to train and test, stabf is y, the others are x, make test dataset 30% of the total dataset (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b7ed17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here. \n",
    "\n",
    "X = STAB.drop('stabf', axis = 1)\n",
    "y = STAB['stabf']\n",
    "\n",
    "X_train, X_test,y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# print(X_train.head())\n",
    "# print(y_train.head())                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1aaaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5497f7ac",
   "metadata": {},
   "source": [
    "### 3. Apply Random Forest to this dataset, predict stabf, show the accuracy. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f77eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# step 1: Import the necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# step 2: Train the Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# step 3: Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# step 4: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# display the accuracy\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc85a2d",
   "metadata": {},
   "source": [
    "### 4. Apply SVM to this dataset, predict stabf, show the accuracy. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc8d2aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Step 1: import libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 2: train the svm model \n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: make predictions\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "# Step 4: evaluate the model\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "\n",
    "print(f'SVM Accuracy: {accuracy_svm:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b2f63d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic_Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Ash_Alcanity</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total_Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid_Phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color_Intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Customer_Segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol  Malic_Acid   Ash  Ash_Alcanity  Magnesium  Total_Phenols  \\\n",
       "0    14.23        1.71  2.43          15.6        127           2.80   \n",
       "1    13.20        1.78  2.14          11.2        100           2.65   \n",
       "2    13.16        2.36  2.67          18.6        101           2.80   \n",
       "3    14.37        1.95  2.50          16.8        113           3.85   \n",
       "4    13.24        2.59  2.87          21.0        118           2.80   \n",
       "\n",
       "   Flavanoids  Nonflavanoid_Phenols  Proanthocyanins  Color_Intensity   Hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   OD280  Proline  Customer_Segment  \n",
       "0   3.92     1065                 1  \n",
       "1   3.40     1050                 1  \n",
       "2   3.17     1185                 1  \n",
       "3   3.45     1480                 1  \n",
       "4   2.93      735                 1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = pd.read_csv(\"Wine.csv\")\n",
    "wine.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61396e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=wine.drop(\"Customer_Segment\",axis=1).values\n",
    "y=wine[\"Customer_Segment\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d2a615",
   "metadata": {},
   "source": [
    "### 5. Standartize the variables X (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17320f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
      "0  0.944087  1.322404  0.212504  0.236362  0.308582 -0.848871  1.201085   \n",
      "1 -0.819194  1.252164  0.296433 -1.599900  0.635702  1.104413 -1.583831   \n",
      "2  0.997219 -0.107312  0.970398  0.921308 -0.245331  1.314141 -0.051993   \n",
      "3 -1.097213  0.708523  1.105498  0.934111 -1.754412  1.697428  1.352325   \n",
      "4 -0.285885  0.964390  0.784895 -1.501700 -0.703486 -1.616879  1.500220   \n",
      "\n",
      "         p4        g1        g2        g3        g4  \n",
      "0 -0.892750  1.485538  0.208525  0.421753 -0.798097  \n",
      "1 -0.621148  1.454570  1.522550  0.979838 -1.426316  \n",
      "2 -0.833814  1.519562  1.073471  1.634525 -0.450236  \n",
      "3  0.000293 -0.590446 -1.416779 -1.615071  0.908748  \n",
      "4  1.334720 -1.394408  1.080738  0.022626  1.046577  \n"
     ]
    }
   ],
   "source": [
    "#Write your code here.\n",
    "# step 1: import necessary libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# step 2: invoke scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# step 3: fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# step 4: transform the test data \n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# viewing the standardized data\n",
    "print(pd.DataFrame(X_train_scaled, columns = X_train.columns).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b08d8d",
   "metadata": {},
   "source": [
    "### 6. Use PCA to reduce standartized X to 2 dimensions (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffc24a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        PC1       PC2\n",
      "0 -0.472485 -1.522133\n",
      "1 -0.870298 -1.093831\n",
      "2  0.316512 -1.880438\n",
      "3  2.509509  1.677581\n",
      "4  1.014759  0.467995\n"
     ]
    }
   ],
   "source": [
    "#Write your code here.\n",
    "# 1: import the necessary libraries\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 2: initialize PCA with the number of components (in this case 2 dim)\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "# 3: fit and transform the training data\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# 4: transform the test data\n",
    "x_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(pd.DataFrame(X_train_pca, columns = ['PC1', 'PC2']).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d388ca7",
   "metadata": {},
   "source": [
    "### 7. Use LDA to reduce standartized X to 2 dimensions (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ca39d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        LD1\n",
      "0  1.825980\n",
      "1  0.652440\n",
      "2  2.911761\n",
      "3 -0.364357\n",
      "4  0.401231\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import the necessary library\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# determine the total unique features/columns \n",
    "n_features = len(y_train.unique())\n",
    "\n",
    "# determine the number of components for LDA\n",
    "n_dimension = min(len(X_train_scaled[0]), n_features - 1)\n",
    "\n",
    "# step 2: Initialize LDA with the number of components\n",
    "lda = LDA(n_components=n_dimension)\n",
    "\n",
    "# step 3: fit and transform the training data\n",
    "X_train_lda = lda.fit_transform(X_train_scaled, y_train)\n",
    "\n",
    "# step 4: transform the test datahttp://localhost:8888/notebooks/Labs/Lab4/Lab4.ipynb#\n",
    "X_test_lda = lda.transform(X_test_scaled)\n",
    "\n",
    "# create appropriate column names for the DataFrame\n",
    "columns = [f'LD{i+1}' for i in range(n_dimension)]\n",
    "\n",
    "# 5 rows of the LDA-transformed training data\n",
    "print(pd.DataFrame(X_train_lda, columns=columns).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553286b",
   "metadata": {},
   "source": [
    "### 8. Apply logistic regression to standartized X, PCA X and LDA X to predict Y, compare the result. Remember to split train and test sets. Set test sets to 20% of whole data. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47fb3020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy on Standardized Data: 0.8130952380952381\n",
      "Logistic Regression Accuracy on PCA Data: 0.6617857142857143\n",
      "Logistic Regression Accuracy on LDA Data: 0.8121428571428572\n"
     ]
    }
   ],
   "source": [
    "#write your code here\n",
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets (20% for testing)\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "X_train_pca_lr, X_test_pca_lr, y_train_pca_lr, y_test_pca_lr = train_test_split(X_train_pca, y_train, test_size=0.2, random_state=42)\n",
    "X_train_lda_lr, X_test_lda_lr, y_train_lda_lr, y_test_lda_lr = train_test_split(X_train_lda, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit and predict on standardized data\n",
    "log_reg.fit(X_train_lr, y_train_lr)\n",
    "y_pred_lr = log_reg.predict(X_test_lr)\n",
    "accuracy_lr = accuracy_score(y_test_lr, y_pred_lr)\n",
    "\n",
    "# Fit and predict on PCA reduced data\n",
    "log_reg.fit(X_train_pca_lr, y_train_pca_lr)\n",
    "y_pred_pca_lr = log_reg.predict(X_test_pca_lr)\n",
    "accuracy_pca_lr = accuracy_score(y_test_pca_lr, y_pred_pca_lr)\n",
    "\n",
    "# Fit and predict on LDA reduced data\n",
    "log_reg.fit(X_train_lda_lr, y_train_lda_lr)\n",
    "y_pred_lda_lr = log_reg.predict(X_test_lda_lr)\n",
    "accuracy_lda_lr = accuracy_score(y_test_lda_lr, y_pred_lda_lr)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy on Standardized Data: {accuracy_lr}\")\n",
    "print(f\"Logistic Regression Accuracy on PCA Data: {accuracy_pca_lr}\")\n",
    "print(f\"Logistic Regression Accuracy on LDA Data: {accuracy_lda_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ebce77",
   "metadata": {},
   "source": [
    "### 9. Apply feature select to the STAB data.(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b6a608c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       tau4      tau1      tau3      tau2        g1\n",
      "0  9.780754  2.959060  8.381025  3.079885  0.650456\n",
      "1  1.369357  9.304097  3.047541  4.902524  0.413441\n",
      "2  1.214518  8.971707  3.046479  8.848428  0.163041\n",
      "3  2.340563  0.716415  4.486641  7.669600  0.446209\n",
      "4  9.857573  3.134112  4.943759  7.608772  0.797110\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "STAB = pd.read_csv(\"smart_grid_stability_augmented.csv\")\n",
    "\n",
    "# Drop 'stab' column\n",
    "STAB = STAB.drop('stab', axis=1)\n",
    "\n",
    "# Separate features and target\n",
    "X = STAB.drop('stabf', axis=1)  # assuming 'stabf' is the target column\n",
    "y = STAB['stabf']\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# print(feature_importances)\n",
    "\n",
    "# Select top N features (e.g., top 5)\n",
    "top_n = 5\n",
    "top_features = feature_importances.head(top_n)['Feature']\n",
    "\n",
    "# Transform the dataset to keep only the top N features\n",
    "X_selected = X[top_features]\n",
    "\n",
    "print(X_selected.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9498fe",
   "metadata": {},
   "source": [
    "### 10. Apply SVM to the edited STAB dataset, compare the result with the previous one.(Remember to split the dataset)(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "923364e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM on Original Dataset:\n",
      "Accuracy: 0.9272222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      stable       0.92      0.88      0.90      6522\n",
      "    unstable       0.93      0.96      0.94     11478\n",
      "\n",
      "    accuracy                           0.93     18000\n",
      "   macro avg       0.93      0.92      0.92     18000\n",
      "weighted avg       0.93      0.93      0.93     18000\n",
      "\n",
      "\n",
      "SVM on Edited Dataset (Selected Features):\n",
      "Accuracy: 0.8122222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      stable       0.79      0.66      0.72      6522\n",
      "    unstable       0.82      0.90      0.86     11478\n",
      "\n",
      "    accuracy                           0.81     18000\n",
      "   macro avg       0.81      0.78      0.79     18000\n",
      "weighted avg       0.81      0.81      0.81     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "STAB = pd.read_csv(\"smart_grid_stability_augmented.csv\")\n",
    "STAB = STAB.drop('stab', axis=1)\n",
    "\n",
    "# Separate features and target\n",
    "X = STAB.drop('stabf', axis=1)  # all features\n",
    "y = STAB['stabf']\n",
    "\n",
    "# Assume X_selected is already created with the top features\n",
    "# If not, use the previous code to create X_selected\n",
    "# Example: X_selected = X[top_features]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train_selected, X_test_selected, y_train_selected, y_test_selected = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "### Step 2: Train SVM Models\n",
    "\n",
    "# Train SVM on the original dataset\n",
    "svm_original = SVC()\n",
    "svm_original.fit(X_train, y_train)\n",
    "y_pred_original = svm_original.predict(X_test)\n",
    "\n",
    "# Train SVM on the edited dataset (selected features)\n",
    "svm_selected = SVC()\n",
    "svm_selected.fit(X_train_selected, y_train_selected)\n",
    "y_pred_selected = svm_selected.predict(X_test_selected)\n",
    "\n",
    "### Step 3: Evaluate Models\n",
    "\n",
    "# Evaluate the SVM model on the original dataset\n",
    "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
    "report_original = classification_report(y_test, y_pred_original)\n",
    "\n",
    "# Evaluate the SVM model on the edited dataset (selected features)\n",
    "accuracy_selected = accuracy_score(y_test_selected, y_pred_selected)\n",
    "report_selected = classification_report(y_test_selected, y_pred_selected)\n",
    "\n",
    "### Step 4: Compare Results\n",
    "\n",
    "print(\"SVM on Original Dataset:\")\n",
    "print(f\"Accuracy: {accuracy_original}\")\n",
    "print(report_original)\n",
    "\n",
    "print(\"\\nSVM on Edited Dataset (Selected Features):\")\n",
    "print(f\"Accuracy: {accuracy_selected}\")\n",
    "print(report_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc28b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db8d7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40210eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f3df7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "STAB = pd.read_csv(\"smart_grid_stability_augmented.csv\")\n",
    "STAB = STAB.drop('stab', axis=1)  # Dropping the 'stab' column\n",
    "X = STAB.drop('stabf', axis=1)    # Features\n",
    "y = STAB['stabf']                 # Target\n",
    "\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "\n",
    "# Train a new model using only the selected features\n",
    "model_selected = DecisionTreeClassifier()\n",
    "model_selected.fit(X_selected, y)\n",
    "\n",
    "# Make predictions on the same data (you should ideally use separate test data for evaluation)\n",
    "y_pred = model_selected.predict(X_selected)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c90847d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[21720     0]\n",
      " [    0 38280]]\n"
     ]
    }
   ],
   "source": [
    "# Obtain confusion matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
