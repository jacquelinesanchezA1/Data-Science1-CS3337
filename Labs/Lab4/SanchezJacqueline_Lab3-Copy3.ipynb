{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ddbc1ee",
   "metadata": {},
   "source": [
    "Jacqueline Sanchez <br> \n",
    "June 21, 2024 <br>\n",
    "COSC 3337 | Summer 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac027c74",
   "metadata": {},
   "source": [
    "## Lab4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce8dd15",
   "metadata": {},
   "source": [
    "### 1. State the difference and similarity of GD and SGD, give each two situitions, one should use GD, the other should use SGD. Explain why. (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ae1bb",
   "metadata": {},
   "source": [
    "<span style = 'color:green'>\n",
    "Gradient Descent processes the entire dataset for each update, leading to stable but slower convergence, making it suitable for small datasets and convex loss functions. Stochastic Gradient Descent, on the other hand, updates parameters more frequently using individual data points which speeds up convergence but introduces more noise. Both methods aim to minimize loss functions by adjusting model parameters based on gradient calculations. Since we are dealing with a smaller dataset, it is preferable to use Gradient Descent for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d63c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77bbd9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAB = pd.read_csv(\"smart_grid_stability_augmented.csv\") # just to observe the data before processing it\n",
    "# STAB.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebbc6ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tau1</th>\n",
       "      <th>tau2</th>\n",
       "      <th>tau3</th>\n",
       "      <th>tau4</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>g1</th>\n",
       "      <th>g2</th>\n",
       "      <th>g3</th>\n",
       "      <th>g4</th>\n",
       "      <th>stabf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.959060</td>\n",
       "      <td>3.079885</td>\n",
       "      <td>8.381025</td>\n",
       "      <td>9.780754</td>\n",
       "      <td>3.763085</td>\n",
       "      <td>-0.782604</td>\n",
       "      <td>-1.257395</td>\n",
       "      <td>-1.723086</td>\n",
       "      <td>0.650456</td>\n",
       "      <td>0.859578</td>\n",
       "      <td>0.887445</td>\n",
       "      <td>0.958034</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.304097</td>\n",
       "      <td>4.902524</td>\n",
       "      <td>3.047541</td>\n",
       "      <td>1.369357</td>\n",
       "      <td>5.067812</td>\n",
       "      <td>-1.940058</td>\n",
       "      <td>-1.872742</td>\n",
       "      <td>-1.255012</td>\n",
       "      <td>0.413441</td>\n",
       "      <td>0.862414</td>\n",
       "      <td>0.562139</td>\n",
       "      <td>0.781760</td>\n",
       "      <td>stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.971707</td>\n",
       "      <td>8.848428</td>\n",
       "      <td>3.046479</td>\n",
       "      <td>1.214518</td>\n",
       "      <td>3.405158</td>\n",
       "      <td>-1.207456</td>\n",
       "      <td>-1.277210</td>\n",
       "      <td>-0.920492</td>\n",
       "      <td>0.163041</td>\n",
       "      <td>0.766689</td>\n",
       "      <td>0.839444</td>\n",
       "      <td>0.109853</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.716415</td>\n",
       "      <td>7.669600</td>\n",
       "      <td>4.486641</td>\n",
       "      <td>2.340563</td>\n",
       "      <td>3.963791</td>\n",
       "      <td>-1.027473</td>\n",
       "      <td>-1.938944</td>\n",
       "      <td>-0.997374</td>\n",
       "      <td>0.446209</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.929381</td>\n",
       "      <td>0.362718</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.134112</td>\n",
       "      <td>7.608772</td>\n",
       "      <td>4.943759</td>\n",
       "      <td>9.857573</td>\n",
       "      <td>3.525811</td>\n",
       "      <td>-1.125531</td>\n",
       "      <td>-1.845975</td>\n",
       "      <td>-0.554305</td>\n",
       "      <td>0.797110</td>\n",
       "      <td>0.455450</td>\n",
       "      <td>0.656947</td>\n",
       "      <td>0.820923</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
       "0  2.959060  3.079885  8.381025  9.780754  3.763085 -0.782604 -1.257395   \n",
       "1  9.304097  4.902524  3.047541  1.369357  5.067812 -1.940058 -1.872742   \n",
       "2  8.971707  8.848428  3.046479  1.214518  3.405158 -1.207456 -1.277210   \n",
       "3  0.716415  7.669600  4.486641  2.340563  3.963791 -1.027473 -1.938944   \n",
       "4  3.134112  7.608772  4.943759  9.857573  3.525811 -1.125531 -1.845975   \n",
       "\n",
       "         p4        g1        g2        g3        g4     stabf  \n",
       "0 -1.723086  0.650456  0.859578  0.887445  0.958034  unstable  \n",
       "1 -1.255012  0.413441  0.862414  0.562139  0.781760    stable  \n",
       "2 -0.920492  0.163041  0.766689  0.839444  0.109853  unstable  \n",
       "3 -0.997374  0.446209  0.976744  0.929381  0.362718  unstable  \n",
       "4 -0.554305  0.797110  0.455450  0.656947  0.820923  unstable  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STAB = pd.read_csv(\"smart_grid_stability_augmented.csv\")\n",
    "STAB = STAB.drop('stab', axis = 1)\n",
    "STAB.head(5) # seems like the response variable is 'stabf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82d96e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 13\n"
     ]
    }
   ],
   "source": [
    "num_rows, num_cols = STAB.shape\n",
    "print(num_rows, num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914bb207",
   "metadata": {},
   "source": [
    "### 2. Seperate the STAB dataset to train and test, stabf is y, the others are x, make test dataset 30% of the total dataset (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b7ed17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n",
    "X = STAB.drop('stabf', axis = 1)\n",
    "y = STAB['stabf']\n",
    "\n",
    "X_train, X_test,y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# print(X_train.head())\n",
    "# print(y_train.head())                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5497f7ac",
   "metadata": {},
   "source": [
    "### 3. Apply Random Forest to this dataset, predict stabf, show the accuracy. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70f77eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "#Write your code here\n",
    "\n",
    "# 1. train the Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# 2. make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# 3. evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc85a2d",
   "metadata": {},
   "source": [
    "### 4. Apply SVM to this dataset, predict stabf, show the accuracy. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b2f63d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic_Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Ash_Alcanity</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total_Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid_Phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color_Intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Customer_Segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol  Malic_Acid   Ash  Ash_Alcanity  Magnesium  Total_Phenols  \\\n",
       "0    14.23        1.71  2.43          15.6        127           2.80   \n",
       "1    13.20        1.78  2.14          11.2        100           2.65   \n",
       "2    13.16        2.36  2.67          18.6        101           2.80   \n",
       "3    14.37        1.95  2.50          16.8        113           3.85   \n",
       "4    13.24        2.59  2.87          21.0        118           2.80   \n",
       "\n",
       "   Flavanoids  Nonflavanoid_Phenols  Proanthocyanins  Color_Intensity   Hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   OD280  Proline  Customer_Segment  \n",
       "0   3.92     1065                 1  \n",
       "1   3.40     1050                 1  \n",
       "2   3.17     1185                 1  \n",
       "3   3.45     1480                 1  \n",
       "4   2.93      735                 1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = pd.read_csv(\"Wine.csv\")\n",
    "wine.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e29ac4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wine.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb0add03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
      "0      2.959060  3.079885  8.381025  9.780754  3.763085 -0.782604 -1.257395   \n",
      "1      9.304097  4.902524  3.047541  1.369357  5.067812 -1.940058 -1.872742   \n",
      "2      8.971707  8.848428  3.046479  1.214518  3.405158 -1.207456 -1.277210   \n",
      "3      0.716415  7.669600  4.486641  2.340563  3.963791 -1.027473 -1.938944   \n",
      "4      3.134112  7.608772  4.943759  9.857573  3.525811 -1.125531 -1.845975   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "60173       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "60174       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "60175       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "60176       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "60177       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "\n",
      "             p4        g1        g2  ...  Magnesium  Total_Phenols  \\\n",
      "0     -1.723086  0.650456  0.859578  ...        NaN            NaN   \n",
      "1     -1.255012  0.413441  0.862414  ...        NaN            NaN   \n",
      "2     -0.920492  0.163041  0.766689  ...        NaN            NaN   \n",
      "3     -0.997374  0.446209  0.976744  ...        NaN            NaN   \n",
      "4     -0.554305  0.797110  0.455450  ...        NaN            NaN   \n",
      "...         ...       ...       ...  ...        ...            ...   \n",
      "60173       NaN       NaN       NaN  ...       95.0           1.68   \n",
      "60174       NaN       NaN       NaN  ...      102.0           1.80   \n",
      "60175       NaN       NaN       NaN  ...      120.0           1.59   \n",
      "60176       NaN       NaN       NaN  ...      120.0           1.65   \n",
      "60177       NaN       NaN       NaN  ...       96.0           2.05   \n",
      "\n",
      "       Flavanoids Nonflavanoid_Phenols  Proanthocyanins  Color_Intensity  \\\n",
      "0             NaN                  NaN              NaN              NaN   \n",
      "1             NaN                  NaN              NaN              NaN   \n",
      "2             NaN                  NaN              NaN              NaN   \n",
      "3             NaN                  NaN              NaN              NaN   \n",
      "4             NaN                  NaN              NaN              NaN   \n",
      "...           ...                  ...              ...              ...   \n",
      "60173        0.61                 0.52             1.06              7.7   \n",
      "60174        0.75                 0.43             1.41              7.3   \n",
      "60175        0.69                 0.43             1.35             10.2   \n",
      "60176        0.68                 0.53             1.46              9.3   \n",
      "60177        0.76                 0.56             1.35              9.2   \n",
      "\n",
      "        Hue  OD280  Proline  Customer_Segment  \n",
      "0       NaN    NaN      NaN               NaN  \n",
      "1       NaN    NaN      NaN               NaN  \n",
      "2       NaN    NaN      NaN               NaN  \n",
      "3       NaN    NaN      NaN               NaN  \n",
      "4       NaN    NaN      NaN               NaN  \n",
      "...     ...    ...      ...               ...  \n",
      "60173  0.64   1.74    740.0               3.0  \n",
      "60174  0.70   1.56    750.0               3.0  \n",
      "60175  0.59   1.56    835.0               3.0  \n",
      "60176  0.60   1.62    840.0               3.0  \n",
      "60177  0.61   1.60    560.0               3.0  \n",
      "\n",
      "[60178 rows x 28 columns]\n",
      "tau1     0\n",
      "tau2     0\n",
      "tau3     0\n",
      "tau4     0\n",
      "p1       0\n",
      "p2       0\n",
      "p3       0\n",
      "p4       0\n",
      "g1       0\n",
      "g2       0\n",
      "g3       0\n",
      "g4       0\n",
      "stab     0\n",
      "stabf    0\n",
      "dtype: int64\n",
      "SVM Accuracy: 0.98\n",
      "SVM Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load datasets\n",
    "STAB = pd.read_csv(\"smart_grid_stability_augmented.csv\")\n",
    "wine = pd.read_csv(\"wine.csv\")  # Ensure you have loaded the wine dataset as well\n",
    "\n",
    "# Concatenate datasets by aligning columns\n",
    "combined_df = pd.concat([STAB, wine], axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "# Print combined dataset to check\n",
    "print(combined_df)\n",
    "\n",
    "# Handle missing values\n",
    "# Remove columns with more than 50% missing values\n",
    "threshold = len(combined_df) * 0.5\n",
    "combined_df = combined_df.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Separate numeric and non-numeric columns\n",
    "numeric_cols = combined_df.select_dtypes(include=['number']).columns\n",
    "non_numeric_cols = combined_df.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "# Impute missing values in numeric columns with the mean\n",
    "combined_df[numeric_cols] = combined_df[numeric_cols].fillna(combined_df[numeric_cols].mean())\n",
    "\n",
    "# Impute missing values in non-numeric columns with the mode\n",
    "for col in non_numeric_cols:\n",
    "    combined_df[col] = combined_df[col].fillna(combined_df[col].mode()[0])\n",
    "\n",
    "# Print the dataframe after handling missing values to verify\n",
    "print(combined_df.isnull().sum())\n",
    "\n",
    "# Split target from the rest of the features\n",
    "X = combined_df.drop('stabf', axis=1)  # Replace 'stabf' with your response variable name if different\n",
    "y = combined_df['stabf']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preparing for svm model training\n",
    "\n",
    "# 1. Train the svm model\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# 2. Make predictions\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "# 3. Evaluate the model\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "print(f'SVM Accuracy: {accuracy_svm:.2f}')\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "STAB = pd.read_csv(\"smart_grid_stability_augmented.csv\")\n",
    "wine = pd.read_csv(\"wine.csv\")  # Ensure you have loaded the wine dataset as well\n",
    "\n",
    "# concatinating datasets by aligning columns\n",
    "combined_df = pd.concat([STAB, wine], axis=0, ignore_index=True, sort=False)\n",
    "# print(combined_df)\n",
    "\n",
    "\n",
    "# handling missing values\n",
    "threshold = len(combined_df) * 0.6 # remove columns with more than 60% missing values\n",
    "combined_df = combined_df.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# separate numeric and non-numeric columns\n",
    "numeric_cols = combined_df.select_dtypes(include=['number']).columns\n",
    "non_numeric_cols = combined_df.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "# impute missing values in numeric columns with the mean\n",
    "combined_df[numeric_cols] = combined_df[numeric_cols].fillna(combined_df[numeric_cols].mean())\n",
    "\n",
    "# impute missing values in non-numeric columns with the mode\n",
    "for col in non_numeric_cols:\n",
    "    combined_df[col] = combined_df[col].fillna(combined_df[col].mode()[0])\n",
    "\n",
    "# print(combined_df.isnull().sum())\n",
    "\n",
    "\n",
    "\n",
    "########### Performing SVM ############\n",
    "\n",
    "# Split target from the rest of the features\n",
    "X = combined_df.drop('stabf', axis=1)  # Replace 'stabf' with your response variable name if different\n",
    "y = combined_df['stabf']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preparing for svm model training\n",
    "\n",
    "# 1. Train the svm model\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# 2. Make predictions\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "# 3. Evaluate the model\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "print(f'SVM Accuracy: {accuracy_svm:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61396e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=wine.drop(\"Customer_Segment\",axis=1).values\n",
    "y=wine[\"Customer_Segment\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d2a615",
   "metadata": {},
   "source": [
    "### 5. Standartize the variables X (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17320f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
      "0  0.944087  1.322404  0.212504  0.236362  0.308582 -0.848871  1.201085   \n",
      "1 -0.819194  1.252164  0.296433 -1.599900  0.635702  1.104413 -1.583831   \n",
      "2  0.997219 -0.107312  0.970398  0.921308 -0.245331  1.314141 -0.051993   \n",
      "3 -1.097213  0.708523  1.105498  0.934111 -1.754412  1.697428  1.352325   \n",
      "4 -0.285885  0.964390  0.784895 -1.501700 -0.703486 -1.616879  1.500220   \n",
      "\n",
      "         p4        g1        g2        g3        g4  \n",
      "0 -0.892750  1.485538  0.208525  0.421753 -0.798097  \n",
      "1 -0.621148  1.454570  1.522550  0.979838 -1.426316  \n",
      "2 -0.833814  1.519562  1.073471  1.634525 -0.450236  \n",
      "3  0.000293 -0.590446 -1.416779 -1.615071  0.908748  \n",
      "4  1.334720 -1.394408  1.080738  0.022626  1.046577  \n"
     ]
    }
   ],
   "source": [
    "#Write your code here.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. invoke scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 2. fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# 3. transform the test data \n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# viewing the standardized data\n",
    "print(pd.DataFrame(X_train_scaled, columns = X_train.columns).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b08d8d",
   "metadata": {},
   "source": [
    "### 6. Use PCA to reduce standartized X to 2 dimensions (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffc24a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        PC1       PC2\n",
      "0 -0.472485 -1.522133\n",
      "1 -0.870298 -1.093831\n",
      "2  0.316512 -1.880438\n",
      "3  2.509509  1.677581\n",
      "4  1.014759  0.467995\n"
     ]
    }
   ],
   "source": [
    "#Write your code here.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. initialize PCA with the number of components (in this case 2 dim)\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "# 2. fit and transform the training data\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# 3. transform the test data\n",
    "x_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(pd.DataFrame(X_train_pca, columns = ['PC1', 'PC2']).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d388ca7",
   "metadata": {},
   "source": [
    "### 7. Use LDA to reduce standartized X to 2 dimensions (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ca39d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        LD1\n",
      "0  1.825980\n",
      "1  0.652440\n",
      "2  2.911761\n",
      "3 -0.364357\n",
      "4  0.401231\n"
     ]
    }
   ],
   "source": [
    "#Write your code here.\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# to perform LDA, we must determine the minimum of n_features-1 or n_classes-1 as it will be the applied dimension.\n",
    "\n",
    "# 1. determine the number of components for LDA\n",
    "n_classes = len(y_train.unique())  \n",
    "# print(n_classes) # there are two unique classes: 'unstable' and 'stable'\n",
    "\n",
    "n_features = X_train_scaled.shape[1]  # number of features in X_train\n",
    "\n",
    "# determining n_component for LDA\n",
    "n_dimension = min(n_features - 1, n_classes - 1) \n",
    "\n",
    "# 2. initialize LDA\n",
    "lda = LDA(n_components=n_dimension)\n",
    "\n",
    "# 3. fit and transform the training data\n",
    "X_train_lda = lda.fit_transform(X_train_scaled, y_train)\n",
    "\n",
    "# 4. transform the test data\n",
    "X_test_lda = lda.transform(X_test_scaled)\n",
    "\n",
    "# create appropriate column names for the dataframe\n",
    "columns = [f'LD{i+1}' for i in range(n_dimension)]\n",
    "\n",
    "# first 5 rows of the LDA-transformed training data\n",
    "print(pd.DataFrame(X_train_lda, columns=columns).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553286b",
   "metadata": {},
   "source": [
    "### 8. Apply logistic regression to standartized X, PCA X and LDA X to predict Y, compare the result. Remember to split train and test sets. Set test sets to 20% of whole data. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47fb3020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy on Standardized X: 0.8130952380952381\n",
      "Logistic Regression Accuracy on PCA X: 0.6617857142857143\n",
      "Logistic Regression Accuracy on LDA X: 0.8121428571428572\n"
     ]
    }
   ],
   "source": [
    "#write your code here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1. initialize Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "# 2. split the data into training and testing sets (20% for testing)\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "X_train_pca_lr, X_test_pca_lr, y_train_pca_lr, y_test_pca_lr = train_test_split(X_train_pca, y_train, test_size=0.2, random_state=42)\n",
    "X_train_lda_lr, X_test_lda_lr, y_train_lda_lr, y_test_lda_lr = train_test_split(X_train_lda, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. fit and predict on standardized data\n",
    "log_reg.fit(X_train_lr, y_train_lr)\n",
    "y_pred_lr = log_reg.predict(X_test_lr)\n",
    "accuracy_lr = accuracy_score(y_test_lr, y_pred_lr)\n",
    "\n",
    "# 4. fit and predict on PCA reduced data\n",
    "log_reg.fit(X_train_pca_lr, y_train_pca_lr)\n",
    "y_pred_pca_lr = log_reg.predict(X_test_pca_lr)\n",
    "accuracy_pca_lr = accuracy_score(y_test_pca_lr, y_pred_pca_lr)\n",
    "\n",
    "# 5. fit and predict on LDA reduced data\n",
    "log_reg.fit(X_train_lda_lr, y_train_lda_lr)\n",
    "y_pred_lda_lr = log_reg.predict(X_test_lda_lr)\n",
    "accuracy_lda_lr = accuracy_score(y_test_lda_lr, y_pred_lda_lr)\n",
    "\n",
    "# X denotes the feature space\n",
    "print(f\"Logistic Regression Accuracy on Standardized X: {accuracy_lr}\")\n",
    "print(f\"Logistic Regression Accuracy on PCA X: {accuracy_pca_lr}\")\n",
    "print(f\"Logistic Regression Accuracy on LDA X: {accuracy_lda_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ebce77",
   "metadata": {},
   "source": [
    "### 9. Apply feature select to the STAB data.(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b6a608c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 7 important features:\n",
      "['tau2', 'tau3', 'tau1', 'tau4', 'g1', 'g2', 'g4']\n",
      "       tau2      tau3      tau1      tau4        g1        g2        g4\n",
      "0  3.079885  8.381025  2.959060  9.780754  0.650456  0.859578  0.958034\n",
      "1  4.902524  3.047541  9.304097  1.369357  0.413441  0.862414  0.781760\n",
      "2  8.848428  3.046479  8.971707  1.214518  0.163041  0.766689  0.109853\n",
      "3  7.669600  4.486641  0.716415  2.340563  0.446209  0.976744  0.362718\n",
      "4  7.608772  4.943759  3.134112  9.857573  0.797110  0.455450  0.820923\n"
     ]
    }
   ],
   "source": [
    "#Write your code here.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 1. Separate features and target\n",
    "X = STAB.drop('stabf', axis=1)  # features (excluding 'stabf' and 'stab')\n",
    "y = STAB['stabf']               # target variable 'stabf'\n",
    "\n",
    "# 2. train a Decision Tree Classifier to get feature importances\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# 4. create a DataFrame for better visualization\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# 5. sort features by importance\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# 6. select top _n features (e.g., top 5)\n",
    "top_n = 7\n",
    "top_features = feature_importances.head(top_n)['Feature'].tolist()\n",
    "\n",
    "\n",
    "print(\"Top 7 important features:\")\n",
    "print(top_features)\n",
    "\n",
    "\n",
    "# transforming the dataset to keep only the top _n features\n",
    "X_selected = X[top_features]\n",
    "\n",
    "print(X_selected.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9498fe",
   "metadata": {},
   "source": [
    "### 10. Apply SVM to the edited STAB dataset, compare the result with the previous one.(Remember to split the dataset)(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "923364e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM on Original Dataset:\n",
      "Accuracy: 0.9272222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      stable       0.92      0.88      0.90      6522\n",
      "    unstable       0.93      0.96      0.94     11478\n",
      "\n",
      "    accuracy                           0.93     18000\n",
      "   macro avg       0.93      0.92      0.92     18000\n",
      "weighted avg       0.93      0.93      0.93     18000\n",
      "\n",
      "\n",
      "SVM on Edited Dataset (Selected Features):\n",
      "Accuracy: 0.885\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      stable       0.86      0.82      0.84      6522\n",
      "    unstable       0.90      0.92      0.91     11478\n",
      "\n",
      "    accuracy                           0.89     18000\n",
      "   macro avg       0.88      0.87      0.87     18000\n",
      "weighted avg       0.88      0.89      0.88     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Write your code here.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "######## (a) preparing data for training ########\n",
    "\n",
    "# (1) Separate features and target\n",
    "X = STAB.drop('stabf', axis=1)  # all other features other than 'stabf'\n",
    "y = STAB['stabf'] # target/response var, 'stabf'\n",
    "\n",
    "\n",
    "# (2) split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train_selected, X_test_selected, y_train_selected, y_test_selected = train_test_split(X_selected,# (* X_selected was aquired from the feature selection made in the cell above!)\n",
    "                                                                                        y, \n",
    "                                                                                        test_size=0.3, \n",
    "                                                                                        random_state=42)\n",
    "\n",
    "####### (b) Train SVM Models ########\n",
    "\n",
    "# train SVM on the original dataset\n",
    "svm_original = SVC()\n",
    "svm_original.fit(X_train, y_train)\n",
    "y_pred_original = svm_original.predict(X_test)\n",
    "\n",
    "# train SVM on the edited dataset (selected features)\n",
    "svm_selected = SVC()\n",
    "svm_selected.fit(X_train_selected, y_train_selected)\n",
    "y_pred_selected = svm_selected.predict(X_test_selected)\n",
    "\n",
    "\n",
    "\n",
    "####### (c) Evaluate Models #######\n",
    "\n",
    "# Evaluate the SVM model on the original dataset\n",
    "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
    "report_original = classification_report(y_test, y_pred_original)\n",
    "\n",
    "# Evaluate the SVM model on the edited dataset (selected features)\n",
    "accuracy_selected = accuracy_score(y_test_selected, y_pred_selected)\n",
    "report_selected = classification_report(y_test_selected, y_pred_selected)\n",
    "\n",
    "\n",
    "####### (d) Compare Results #######\n",
    "\n",
    "print(\"SVM on Original Dataset:\")\n",
    "print(f\"Accuracy: {accuracy_original}\")\n",
    "print(report_original)\n",
    "\n",
    "print(\"\\nSVM on Edited Dataset (Selected Features):\")\n",
    "print(f\"Accuracy: {accuracy_selected}\")\n",
    "print(report_selected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c748fe",
   "metadata": {},
   "source": [
    "note to self(findings) <br>\n",
    "I noticed that SVM applied to the selected feature set X yielded a lower score than when applied to the original dataset. This could be due to the Curse of Dimensionality, as reducing dataset components may impair the creation of optimal separating hyperplanes and lead to decreased model performance in high-dimensional data scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
